{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b868a8b9-5360-46c9-939f-cb1fd1a1cc7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from attrs import define, Factory, asdict\n",
    "\n",
    "from typing import (\n",
    "    Union,\n",
    "    Sequence,\n",
    "    Iterable,\n",
    "    Optional,\n",
    ")\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950612b8-38bd-428e-8605-613ddd4dcc13",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f362f0c-71ba-4f05-bb5c-813c97f9bb13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data paths\n",
    "\n",
    "DATA_PATH = Path(\".\")/\"input\"\n",
    "TITLE_DATASET = DATA_PATH/\"all_titles.txt\"\n",
    "\n",
    "# Tokenizer paths\n",
    "\n",
    "TOKENIZER_PATH = Path(\".\")/\"model\"/\"tokenizer\"\n",
    "TOKENIZER_TRAINING_CONFIG = TOKENIZER_PATH/\"training_config.json\"\n",
    "PRETRAINED_TOKENIZER_DIR = TOKENIZER_PATH/\"pretrained_tokenizer\"\n",
    "\n",
    "# Tokenizer initial parameters\n",
    "\n",
    "VOCAB_SIZE = 1528  # Number of base characters (see EDA) + four special tokens + 1000 merges\n",
    "UNK = \"[UNK]\"\n",
    "CLS = \"[CLS]\"\n",
    "SEP = \"[SEP]\"\n",
    "PAD = \"[PAD]\"\n",
    "SPECIAL_TOKENS = [PAD, CLS, SEP, UNK]\n",
    "CONTINUING_SUBWORD_PREFIX = \"##\"\n",
    "PADDING_SIDE = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaef105-39ad-414d-b7b3-630f8ad4c8d9",
   "metadata": {},
   "source": [
    "## Tokenizer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ddb79-c38c-4815-8735-13899512a421",
   "metadata": {},
   "source": [
    "#### Tokenizer Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e4ce55-ac69-4281-adfb-01cdc924b8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@define(kw_only=True)\n",
    "class TokenizerTrainingConfig:\n",
    "    dataset_path: Union[str, os.PathLike] = TITLE_DATASET\n",
    "    output_dir: Union[str, os.PathLike] = PRETRAINED_TOKENIZER_DIR\n",
    "    vocab_size: int = VOCAB_SIZE\n",
    "    special_tokens: Sequence[str] = Factory(\n",
    "        lambda: SPECIAL_TOKENS\n",
    "    )\n",
    "    unk_token: str = UNK\n",
    "    cls_token: str = CLS\n",
    "    sep_token: str = SEP\n",
    "    pad_token: str = PAD\n",
    "    continuing_subword_prefix: str = CONTINUING_SUBWORD_PREFIX\n",
    "    padding_side: str = PADDING_SIDE\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, config_json: Union[str, os.PathLike]):\n",
    "        if not os.path.exists(config_json):\n",
    "            raise FileNotFoundError(f\"Couldn't find {config_json}\")\n",
    "        with open(config_json, \"r\") as infile:\n",
    "            config = json.load(infile)\n",
    "            config[\"dataset_path\"] = Path(config[\"dataset_path\"])\n",
    "            config[\"output_dir\"] = Path(config[\"output_dir\"])\n",
    "            return cls(**config)\n",
    "        \n",
    "    def to_json(self, config_json: Union[str, os.PathLike]):\n",
    "        config_dict = asdict(self)\n",
    "        config_dict[\"dataset_path\"] = str(self.dataset_path)\n",
    "        config_dict[\"output_dir\"] = str(self.output_dir)\n",
    "        \n",
    "        with open(config_json, \"w\") as outfile:\n",
    "            json.dump(config_dict, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e62288f-5bab-445e-8359-dc73aa31c63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(TOKENIZER_TRAINING_CONFIG):\n",
    "    tokenizer_training_config = TokenizerTrainingConfig()\n",
    "    tokenizer_training_config.to_json(TOKENIZER_TRAINING_CONFIG)\n",
    "else:\n",
    "    tokenizer_training_config = TokenizerTrainingConfig.from_json(TOKENIZER_TRAINING_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9995f-53dc-440b-baef-838a740da4c2",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f02dfc0-8a44-45f8-870e-36b8ad1b2755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_tokenizer(config: TokenizerTrainingConfig):\n",
    "    if not os.path.exists(config.dataset_path):\n",
    "        raise FileNotFoundError(f\"Couldn't find {config.dataset_path}\")\n",
    "    \n",
    "    def load_data(data_file: Union[str, os.PathLike],\n",
    "                  encoding: Optional[str] = \"utf-8\") -> Iterable[str]:\n",
    "        with open(data_file, \"r\", encoding=encoding) as infile:\n",
    "            for line in infile:\n",
    "                yield line.strip()\n",
    "                \n",
    "    tokenizer = Tokenizer(models.BPE(\n",
    "        unk_token=config.unk_token,\n",
    "        continuing_subword_prefix=config.continuing_subword_prefix\n",
    "    ))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    \n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=config.vocab_size,\n",
    "        special_tokens=config.special_tokens,\n",
    "        continuing_subword_prefix=config.continuing_subword_prefix\n",
    "    )\n",
    "    tokenizer.train_from_iterator(load_data(config.dataset_path), trainer=trainer)\n",
    "    \n",
    "    cls_token_id = tokenizer.token_to_id(config.cls_token)\n",
    "    sep_token_id = tokenizer.token_to_id(config.sep_token)\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (config.cls_token, cls_token_id),\n",
    "            (config.sep_token, sep_token_id),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        unk_token=config.unk_token,\n",
    "        pad_token=config.pad_token,\n",
    "        sep_token=config.sep_token,\n",
    "        padding_side=config.padding_side,\n",
    "    )\n",
    "    wrapped_tokenizer.save_pretrained(config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b2cf44-0fca-460d-81d6-65cf8c934281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_tokenizer(tokenizer_training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9d496-b21c-42c5-8bee-f570863828ac",
   "metadata": {},
   "source": [
    "#### Basic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90d4bdc-4252-4387-a659-96592ec46f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(PRETRAINED_TOKENIZER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac6c8245-2ad4-46c4-8cbb-0898e6615780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 488, 346, 1233, 339, 540, 1490, 333, 518, 25, 921, 325, 1287, 1104, 26, 578, 164, 2\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"–ì–∞–¥–∞–µ–º –Ω–∞ —Å–≤–∞–¥–µ–±–Ω—ã—Ö –±—É–∫–µ—Ç–∞—Ö: –∫–æ–≥–¥–∞ —Ç—ã –≤—ã–π–¥–µ—à—å –∑–∞–º—É–∂? üíê\"\n",
    "token_ids = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "print(\", \".join(list(map(str, token_ids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0689da2-cf96-489b-9975-47bbd29329c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –ì–∞–¥–∞–µ–º –Ω–∞ —Å–≤–∞–¥##–µ–±##–Ω—ã—Ö –±—É–∫##–µ—Ç##–∞—Ö: –∫–æ–≥–¥–∞ —Ç—ã –≤—ã–π–¥–µ—à—å –∑–∞–º—É–∂? ÔøΩ##ÔøΩ\n"
     ]
    }
   ],
   "source": [
    "restored_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "print(restored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5ab66c-20f7-4b7e-92e1-923fb5019b52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\t1\n",
      "–ì–∞–¥–∞–µ–º\t488\n",
      "–Ω–∞\t346\n",
      "—Å–≤–∞–¥\t1233\n",
      "##–µ–±\t339\n",
      "##–Ω—ã—Ö\t540\n",
      "–±—É–∫\t1490\n",
      "##–µ—Ç\t333\n",
      "##–∞—Ö\t518\n",
      ":\t25\n",
      "–∫–æ–≥–¥–∞\t921\n",
      "—Ç—ã\t325\n",
      "–≤—ã–π–¥–µ—à—å\t1287\n",
      "–∑–∞–º—É–∂\t1104\n",
      "?\t26\n",
      "ÔøΩ\t578\n",
      "##ÔøΩ\t164\n",
      "[SEP]\t2\n"
     ]
    }
   ],
   "source": [
    "for _id in token_ids:\n",
    "    print(f\"{tokenizer.decode([_id], skip_special_tokens=False).strip()}\\t{_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc926922-87ba-4e70-8d77-1bd4f81d09d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search-suggestions",
   "language": "python",
   "name": "search-suggestions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
