{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18d1679-46bc-463e-9d76-c14018fdf24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from attrs import define, Factory, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import lr_scheduler as torch_lr_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import (\n",
    "    Union,\n",
    "    Iterable,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Any,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa31f7-fd6c-4076-805c-9d42a7712a92",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05573cf9-19f8-47b4-a0b0-c44be08db521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data paths\n",
    "\n",
    "DATA_PATH = Path(\".\")/\"input\"\n",
    "ALL_TERMS = DATA_PATH/\"all_titles.txt\"\n",
    "TRAIN_TERMS = DATA_PATH/\"train_titles.txt\"\n",
    "VAL_TERMS = DATA_PATH/\"validation_titles.txt\"\n",
    "\n",
    "# Tokenizer\n",
    "\n",
    "PRETRAINED_TOKENIZER = Path(\".\")/\"model\"/\"tokenizer\"/\"pretrained_tokenizer\"\n",
    "\n",
    "# Model\n",
    "\n",
    "MODEL_PATH = Path(\".\")/\"model\"/\"generator\"\n",
    "MODEL_CONFIG = MODEL_PATH/\"config.json\"\n",
    "MODEL_WEIGTHS = MODEL_PATH/\"title_generator.sav\"\n",
    "MODEL_TRAINING_SNAPSHOT = MODEL_PATH/\"snapshot\"/\"generator_training_snapshot.sav\"\n",
    "BEST_METRICS = MODEL_PATH/\"snapshot\"/\"best_metrics.json\"\n",
    "TRAINING_LOG = MODEL_PATH/\"snapshot\"/\"training_log.txt\"\n",
    "\n",
    "# Tensorboard\n",
    "\n",
    "TENSORBOARD_PATH = Path(\".\")/\"img\"/\"tensorboard\"\n",
    "TRAIN = \"Train\"\n",
    "VALIDATION = \"Validation\"\n",
    "LOSS = \"Loss\"\n",
    "\n",
    "# Model parameters\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 24  # see query length distribution histogram\n",
    "VOCAB_SIZE = 2500  # see the pre-trained tokenizer parameters\n",
    "HEAD_NUM = 6\n",
    "EMBEDDING_DIM = 128 * HEAD_NUM\n",
    "LAYER_NUM = 6\n",
    "DROPOUT = 0.2\n",
    "\n",
    "EPOCH_NUM = 1000\n",
    "START_EPOCH = 0\n",
    "LOG_INTERVAL = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349f0d8-d856-4807-9274-8b07d150716b",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02f0556-90a7-4600-b400-aaeced43863f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@define(kw_only=True)\n",
    "class Config:\n",
    "    attention_head_num: int = HEAD_NUM\n",
    "    embedding_dim: int = EMBEDDING_DIM\n",
    "    decoder_layer_num: int = LAYER_NUM\n",
    "    max_sequence_length: int = MAX_SEQ_LEN\n",
    "    batch_size: int = BATCH_SIZE\n",
    "    dropout: int = DROPOUT\n",
    "    vocab_size: int = VOCAB_SIZE\n",
    "    tokenizer: Union[str, os.PathLike] = PRETRAINED_TOKENIZER\n",
    "    train_data: Union[str, os.PathLike] = TRAIN_TERMS\n",
    "    validation_data: Union[str, os.PathLike] = VAL_TERMS\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, config_json: Union[str, os.PathLike]):\n",
    "        if not os.path.exists(config_json):\n",
    "            raise FileNotFoundError(f\"Couldn't find {config_json}\")\n",
    "            \n",
    "        with open(config_json, \"r\") as infile:\n",
    "            config = json.load(infile)\n",
    "            config[\"tokenizer\"] = Path(config[\"tokenizer\"])\n",
    "            config[\"train_data\"] = Path(config[\"train_data\"])\n",
    "            config[\"validation_data\"] = Path(config[\"validation_data\"])\n",
    "            return cls(**config)\n",
    "        \n",
    "    def to_json(self, config_json: Union[str, os.PathLike]):\n",
    "        config_dict = asdict(self)\n",
    "        config_dict[\"tokenizer\"] = str(self.tokenizer)\n",
    "        config_dict[\"train_data\"] = str(self.train_data)\n",
    "        config_dict[\"validation_data\"] = str(self.validation_data)\n",
    "        \n",
    "        with open(config_json, \"w\") as outfile:\n",
    "            json.dump(config_dict, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd691cb4-231c-482f-a1fc-d925ef67fb27",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f1828a-8209-47cd-8f29-052c3a7a7b70",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d078b4f6-7e73-4455-9158-d9736067892d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(data_path: Union[str, os.PathLike],\n",
    "              encoding: Optional[str] = \"utf-8\") -> Iterable[str]:\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Couldn't find {data_path}\")\n",
    "    with open(data_path, \"r\", encoding=encoding) as infile:\n",
    "        for line in infile:\n",
    "            yield line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c70a551-b5f5-4383-b168-617bf6da3a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dump_data_to_txt(data: Sequence[str],\n",
    "                     data_path: Union[str, os.PathLike],\n",
    "                     encoding: Optional[str] = \"utf-8\"):\n",
    "    with open(data_path, \"w\", encoding=encoding) as outfile:\n",
    "        for item in data:\n",
    "            outfile.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ecff37-1394-4548-906c-c1d3f41136e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_is_split = os.path.exists(TRAIN_TERMS) and os.path.exists(VAL_TERMS)\n",
    "\n",
    "if not dataset_is_split:\n",
    "    data = list(load_data(ALL_TERMS))\n",
    "    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    dump_data_to_txt(train_data, TRAIN_TERMS)\n",
    "    dump_data_to_txt(val_data, VAL_TERMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc828c0-c61e-4b9c-94ca-b98ef672460d",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61ac92f3-e7f0-47c8-83d9-19918e93a0f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SampleSet:\n",
    "    def __init__(self,\n",
    "                 data: Sequence[str],\n",
    "                 tokenizer: PreTrainedTokenizerFast,\n",
    "                 max_seq_len: int, batch_size: int,\n",
    "                 use_cuda: Optional[bool] = True,\n",
    "                 gpu_num: Optional[int] = 1):\n",
    "        features, targets = self.make_samples(data, tokenizer, max_seq_len)\n",
    "        kwargs = self.make_kwargs(use_cuda, gpu_num)\n",
    "        \n",
    "        self.set = TensorDataset(features, targets)\n",
    "        self.loader = DataLoader(self.set, batch_size=batch_size,\n",
    "                                drop_last=True, shuffle=True, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_samples(data: Sequence[str], tokenizer: PreTrainedTokenizerFast,\n",
    "                     max_seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        features, targets = [], []\n",
    "        for item in data:\n",
    "            token_ids = tokenizer.encode(item, add_special_tokens=True, padding=\"max_length\",\n",
    "                                         truncation=True, max_length=max_seq_len + 1,\n",
    "                                         return_tensors=\"pt\").squeeze(dim=0)\n",
    "            features.append(token_ids[:-1])\n",
    "            targets.append(token_ids[1:])\n",
    "            \n",
    "        features = torch.stack(features)\n",
    "        targets = torch.stack(targets)\n",
    "        \n",
    "        return features, targets\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_kwargs(use_cuda: Optional[bool] = True, gpu_num: Optional[int] = 1):\n",
    "        print(f\"Using CUDA: {use_cuda}\")\n",
    "        kwargs = {\n",
    "            \"num_workers\": 4 * gpu_num,\n",
    "            \"pin_memory\": True\n",
    "        } if use_cuda else {}\n",
    "        return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3be9a5e2-3871-48d6-ac32-fb2e0e92f370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@define(kw_only=True)\n",
    "class Dataset:\n",
    "    train: SampleSet\n",
    "    validation: SampleSet\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config: Config):\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained(config.tokenizer)\n",
    "        train_data = list(load_data(config.train_data))\n",
    "        val_data = list(load_data(config.validation_data))\n",
    "        return cls(\n",
    "            train=SampleSet(data=train_data, tokenizer=tokenizer,\n",
    "                            max_seq_len=config.max_sequence_length,\n",
    "                            batch_size=config.batch_size),\n",
    "            validation=SampleSet(data=val_data, tokenizer=tokenizer,\n",
    "                                 max_seq_len=config.max_sequence_length,\n",
    "                                 batch_size=config.batch_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1007d06c-87c8-4c0e-933b-67f27b787fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(MODEL_CONFIG):\n",
    "    config = Config.from_json(MODEL_CONFIG)\n",
    "else:\n",
    "    config = Config()\n",
    "    config.to_json(MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2cdd16-e25c-4377-abc6-79350c524a80",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294047d-4438-46cc-8fd5-4d7bb6928993",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f80124-7629-40bf-a41b-05f9cdb34486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, head_dim: int, max_seq_len: int, dropout: float):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        # Query, Key, Value layers\n",
    "        self.query = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.key = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        \n",
    "        # A lower triangular matrix to depreciate context from the right\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(max_seq_len, max_seq_len)))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        batch_size, seq_len, embedding_dim = _input.shape\n",
    "        k = self.key(_input)\n",
    "        q = self.query(_input)\n",
    "        \n",
    "        # Attention scores --> batch_size x seq_len x seq_len\n",
    "        attention_scores = q @ k.transpose(-2, -1) * embedding_dim ** -0.5\n",
    "        # Set forward context to -inf\n",
    "        attention_scores = attention_scores.masked_fill(\n",
    "            self.mask[:seq_len, :seq_len] == 0,\n",
    "            float(\"-inf\")\n",
    "        )\n",
    "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "        \n",
    "        # Aggregate values by attention\n",
    "        v = self.value(_input)\n",
    "        attention = attention_scores @ v  # batch_size x seq_len x head_dim\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e8be86-eb03-4641-b688-377e388a24fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, head_dim: int, head_num: int,\n",
    "                 max_seq_len: int, dropout: float):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            AttentionHead(embedding_dim=embedding_dim, head_dim=head_dim,\n",
    "                          max_seq_len=max_seq_len, dropout=dropout)\n",
    "            for _ in range(head_num)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(head_dim * head_num, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        _output = torch.cat(\n",
    "            [\n",
    "                attention_head(_input)\n",
    "                for attention_head\n",
    "                in self.attention_heads\n",
    "            ],\n",
    "            dim=-1\n",
    "        )\n",
    "        _output = self.dropout(self.output_layer(_output))\n",
    "        return _output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85fda973-4c69-4bd3-a2b0-86039bb9cb42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, dropout: float):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, _input):\n",
    "        return self.feed_forward(_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e89aa4b-009f-4b14-a43c-9a58662bc0af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, head_num: int, max_seq_len: int, embedding_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        head_dim = embedding_dim // head_num\n",
    "        self.self_attention = MultiHeadAttention(\n",
    "            head_num=head_num,\n",
    "            head_dim=head_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            max_seq_len=max_seq_len,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward_layer = FeedForward(embedding_dim=embedding_dim, dropout=dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, _input):\n",
    "        x = _input + self.self_attention(self.layer_norm1(_input))\n",
    "        x = x + self.feed_forward_layer(self.layer_norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a3f571-4c90-4aef-bdeb-2a88b0635bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, max_seq_len: int,\n",
    "                 head_num: int, decoder_layer_num: int, dropout: float,\n",
    "                 use_cuda: Optional[bool] = True):\n",
    "        super(Transformer, self).__init__()\n",
    "        # Model parameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.head_num = head_num\n",
    "        self.decoder_layer_num = decoder_layer_num\n",
    "        self.dropout = dropout\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        # Token and positional embedding lookup tables \n",
    "        self.token_embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(self.max_seq_len, self.embedding_dim)\n",
    "        \n",
    "        # Decoder transformer blocks\n",
    "        self.decoder = nn.Sequential(*[\n",
    "                TransformerBlock(\n",
    "                    head_num=self.head_num,\n",
    "                    max_seq_len=self.max_seq_len,\n",
    "                    embedding_dim=self.embedding_dim,\n",
    "                    dropout=self.dropout\n",
    "                )\n",
    "                for _ in range(self.decoder_layer_num)\n",
    "        ])\n",
    "        \n",
    "        # LM head\n",
    "        self.lm_head = nn.Linear(self.embedding_dim, self.vocab_size)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config: Config):\n",
    "        return cls(\n",
    "            vocab_size=config.vocab_size,\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            max_seq_len=config.max_sequence_length,\n",
    "            head_num=config.attention_head_num,\n",
    "            decoder_layer_num=config.decoder_layer_num,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, indexed_seq):\n",
    "        batch_size, seq_len = indexed_seq.shape\n",
    "        \n",
    "        token_embeddings = self.token_embedding(indexed_seq)\n",
    "        positional_embeddings = self.position_embedding(torch.arange(seq_len,\n",
    "                                                                     device=torch.device(\"cuda\") if self.use_cuda \\\n",
    "                                                                     else torch.device(\"cpu\")))\n",
    "\n",
    "        x = token_embeddings + positional_embeddings\n",
    "        x = self.decoder(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb96a54-a1c9-4620-8e3e-690c126cef4a",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4556507-322f-4176-b0ce-854bcd50937d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@define(kw_only=True)\n",
    "class TransformerTrainer:\n",
    "    dataset: Dataset\n",
    "    model: Transformer\n",
    "    optimizer: AdamW\n",
    "    lr_scheduler: torch_lr_scheduler.CosineAnnealingLR\n",
    "    tensorboard_writer: SummaryWriter\n",
    "    criterion: nn.CrossEntropyLoss = Factory(\n",
    "        lambda: nn.CrossEntropyLoss()\n",
    "    )\n",
    "    best_metrics: dict = Factory(\n",
    "        lambda: {\"train_loss\": float(\"inf\"), \"validation_loss\": float(\"inf\")}\n",
    "    )\n",
    "    device: torch.device = Factory(\n",
    "        lambda: torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    )\n",
    "    \n",
    "    @classmethod\n",
    "    def init_trainer(cls, config: Config,\n",
    "                     device: torch.device,\n",
    "                     training_snapshot: Optional[Union[str, os.PathLike]] = None,\n",
    "                     metrics: Optional[Union[str, os.PathLike]] = None):\n",
    "        # Load state checkpoint if present\n",
    "        snapshot = None\n",
    "        if training_snapshot is not None:\n",
    "            if not os.path.exists(training_snapshot):\n",
    "                raise FileNotFoundError(f\"File {training_snapshot} doesn't exist :/ Check your snapshots!\")\n",
    "            snapshot = torch.load(training_snapshot)\n",
    "            \n",
    "        # Dataset\n",
    "        dataset = Dataset.from_config(config)\n",
    "        # Model\n",
    "        model = Transformer.from_config(config)\n",
    "        if snapshot is not None:\n",
    "            model.load_state_dict(snapshot[\"model\"])\n",
    "        if device.type == \"cuda\":\n",
    "            model.cuda()\n",
    "        # Optimizer        \n",
    "        optimizer = AdamW(model.parameters(), lr=3e-4)\n",
    "        if snapshot is not None:\n",
    "            optimizer.load_state_dict(snapshot[\"optimizer\"])\n",
    "        # Learning rate scheduler\n",
    "        last_epoch = 0 if snapshot is None else snapshot[\"last_epoch\"]\n",
    "        lr_scheduler = torch_lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                            T_max=(EPOCH_NUM - last_epoch) * len(dataset.train.loader),\n",
    "                                                            eta_min=1e-5)\n",
    "        if snapshot is not None:\n",
    "            lr_scheduler.load_state_dict(snapshot[\"lr_scheduler\"])\n",
    "            \n",
    "        # Load the previous best metrics if present\n",
    "        best_metrics = None\n",
    "        if metrics is not None:\n",
    "            if not os.path.exists(metrics):\n",
    "                raise FileNotFoundError(f\"File {metrics} doesn't exist :/ Do you really save the metrics, huh?\")\n",
    "            with open(metrics, \"r\") as infile:\n",
    "                best_metrics = json.load(infile)\n",
    "                \n",
    "        # Tensorboard summary writer\n",
    "        tensorboard_writer = SummaryWriter(TENSORBOARD_PATH, purge_step=last_epoch + 1, flush_secs=5)\n",
    "        \n",
    "        kwargs = {\n",
    "            \"dataset\": dataset,\n",
    "            \"model\": model,\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": lr_scheduler,\n",
    "            \"tensorboard_writer\": tensorboard_writer,\n",
    "            \"device\": device\n",
    "        }\n",
    "        if best_metrics is not None:\n",
    "            kwargs[\"best_metrics\"] = best_metrics\n",
    "        \n",
    "        return cls(**kwargs)\n",
    "    \n",
    "    def fit(self,\n",
    "            start_epoch: Optional[int] = START_EPOCH,\n",
    "            epoch_num: Optional[int] = EPOCH_NUM,\n",
    "            log_interval: Optional[int] = LOG_INTERVAL,\n",
    "            best_model_dump: Optional[Union[str, os.PathLike]] = MODEL_WEIGTHS,\n",
    "            model_snapshot_dump: Optional[Union[str, os.PathLike]] = MODEL_TRAINING_SNAPSHOT,\n",
    "            training_log: Optional[Union[str, os.PathLike]] = TRAINING_LOG,\n",
    "            best_metrics_dump: Optional[Union[str, os.PathLike]] = BEST_METRICS):\n",
    "        # Mark the processing of one batch as a completed step\n",
    "        num_steps = (epoch_num - start_epoch) * len(self.dataset.train.loader)\n",
    "        progress_bar = tqdm(range(num_steps), position=0, leave=True)\n",
    "        \n",
    "        for epoch in range(start_epoch, epoch_num, 1):\n",
    "            self.train_epoch(epoch, training_log, progress_bar)\n",
    "            \n",
    "            if epoch == 0 or (epoch + 1) % log_interval == 0:\n",
    "                self.validate(epoch, best_model_dump, model_snapshot_dump, best_metrics_dump)\n",
    "        \n",
    "    def train_epoch(self, epoch: int,\n",
    "                    training_log: Union[str, os.PathLike], progress_bar: tqdm):\n",
    "        self.model.train()\n",
    "        \n",
    "        epoch_loss = []\n",
    "        for _input, _target in self.dataset.train.loader:\n",
    "            if self.device.type == \"cuda\":\n",
    "                _input, _target = _input.cuda(), _target.cuda()\n",
    "            predicted = self.model(_input)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = predicted.shape\n",
    "            predicted = torch.reshape(predicted, (batch_size * seq_len, vocab_size))\n",
    "            _target = torch.reshape(_target, (batch_size * seq_len,))\n",
    "            loss = self.criterion(predicted, _target)\n",
    "            epoch_loss.append(loss.cpu().item())\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            \n",
    "        mean_epoch_loss = sum(epoch_loss) / len(epoch_loss)\n",
    "        self.best_metrics[\"train_loss\"] = min(mean_epoch_loss, self.best_metrics[\"train_loss\"])\n",
    "        self.tensorboard_writer.add_scalar(f\"{LOSS}/{TRAIN}\", mean_epoch_loss, epoch + 1)\n",
    "        with open(training_log, \"a\") as outfile:\n",
    "            outfile.write(f\"Epoch #{epoch + 1}\\nMean epoch loss: {mean_epoch_loss}\\n\\n\")\n",
    "            \n",
    "    def validate(self, epoch: int, best_model_dump: Union[str, os.PathLike], \n",
    "                 model_snapshot_dump: Union[str, os.PathLike], best_metrics_dump: Union[str, os.PathLike]):\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            \n",
    "            for _input, _target in self.dataset.validation.loader:\n",
    "                if self.device.type == \"cuda\":\n",
    "                    _input, _target = _input.cuda(), _target.cuda()\n",
    "                    \n",
    "                predicted = self.model(_input)\n",
    "                batch_size, seq_len, vocab_size = predicted.shape\n",
    "                predicted = torch.reshape(predicted, (batch_size * seq_len, vocab_size))\n",
    "                _target = torch.reshape(_target, (batch_size * seq_len,))\n",
    "                loss = self.criterion(predicted, _target)\n",
    "                val_loss.append(loss.cpu().item())\n",
    "                \n",
    "        mean_val_loss = sum(val_loss) / len(val_loss)\n",
    "        self.tensorboard_writer.add_scalar(f\"{LOSS}/{VALIDATION}\", mean_val_loss, epoch + 1)\n",
    "        # Save model training snapshot\n",
    "        snapshot = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"lr_scheduler\": self.lr_scheduler.state_dict(),\n",
    "            \"last_epoch\": epoch + 1\n",
    "        }\n",
    "        self.atomic_write(snapshot, model_snapshot_dump)\n",
    "        # Save new best weights\n",
    "        if mean_val_loss < self.best_metrics[\"validation_loss\"]:\n",
    "            self.atomic_write(self.model.state_dict(), best_model_dump)\n",
    "            self.best_metrics[\"validation_loss\"] = mean_val_loss\n",
    "            print(f\"Epoch #{epoch + 1}\\n*** New best validation loss: {mean_val_loss}!\\n\")\n",
    "        self.atomic_write(self.best_metrics, best_metrics_dump, _format=\"json\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def atomic_write(_object: Any, _file: Union[str, os.PathLike],\n",
    "                     _format: Optional[str] = \"torch\"):\n",
    "        if type(_file) is str:\n",
    "            _file = Path(_file)\n",
    "        base_folder = _file.parent\n",
    "        tmp_filename = f\"{_file.name.split('.')[0]}.tmp.{_file.name.split('.')[-1]}\"\n",
    "        tmp_file = base_folder/tmp_filename\n",
    "        \n",
    "        mode = \"wb\" if _format == \"torch\" else \"w\"\n",
    "        with open(tmp_file, mode) as outfile:\n",
    "            if _format == \"json\":\n",
    "                json.dump(_object, outfile, ensure_ascii=False, indent=4)\n",
    "            elif _format == \"torch\":\n",
    "                torch.save(_object, outfile)\n",
    "            outfile.flush()\n",
    "            os.fsync(outfile.fileno())\n",
    "        os.replace(tmp_file, _file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52c43e-2ad2-426d-9ae9-488937baa557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = TransformerTrainer.init_trainer(config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402cef02-c13a-45bf-a66f-86d3f2078bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252af410-f8cf-4bad-9c00-daf888bd570e",
   "metadata": {},
   "source": [
    "#### Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcc60313-9dd5-48bc-a00a-5948f186f687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@define(kw_only=True)\n",
    "class TheGirlPT:\n",
    "    tokenizer: PreTrainedTokenizerFast\n",
    "    model: Transformer\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        pretrained_tokenizer: Union[str, os.PathLike],\n",
    "                        model_config: Union[str, os.PathLike],\n",
    "                        pretrained_model: Union[str, os.PathLike],\n",
    "                        use_cuda: Optional[bool] = True):\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained(pretrained_tokenizer)\n",
    "        \n",
    "        config = Config.from_json(model_config)\n",
    "        model = Transformer.from_config(config)\n",
    "        device = torch.device(\n",
    "            \"cpu\"\n",
    "            if not use_cuda or not torch.cuda.is_available()\n",
    "            else \"cuda\"\n",
    "        )\n",
    "        model.use_cuda = device.type == \"cuda\"\n",
    "        model.load_state_dict(torch.load(pretrained_model))\n",
    "        if model.use_cuda:\n",
    "            model.cuda()\n",
    "        model.eval()\n",
    "        \n",
    "        return cls(tokenizer=tokenizer, model=model)\n",
    "    \n",
    "    def encode_sequence(self, _input: str) -> torch.Tensor:\n",
    "        token_ids = self.tokenizer.encode(_input, add_special_tokens=True, padding=False,\n",
    "                                          truncation=True, max_length=self.model.max_seq_len,\n",
    "                                          return_tensors=\"pt\")[:, :-1]\n",
    "        return token_ids\n",
    "    \n",
    "    def tokenize_sequence(self, _input: str) -> Sequence[Tuple[int, str]]:\n",
    "        token_ids = self.tokenizer.encode(_input, add_special_tokens=False)\n",
    "        tokens = [\n",
    "            (_id, self.tokenizer.decode([_id], skip_special_tokens=True))\n",
    "            for _id\n",
    "            in token_ids\n",
    "        ]\n",
    "        return tokens\n",
    "    \n",
    "    def generate_sample(self,\n",
    "                        seed_phrase: Optional[str] = \"\",\n",
    "                        max_new_tokens: Optional[int] = 24,\n",
    "                        temperature: Optional[float] = 1.0) -> str:\n",
    "        seed_ids = self.encode_sequence(seed_phrase)\n",
    "        if self.model.use_cuda:\n",
    "            seed_ids = seed_ids.cuda()\n",
    "            \n",
    "        new_tokens_num = min(self.model.max_seq_len - seed_ids.shape[-1], max_new_tokens)\n",
    "        for _ in range(new_tokens_num):\n",
    "            logp_next = self.model(seed_ids)\n",
    "            p_next = F.softmax(logp_next / temperature, dim=-1).cpu().data.numpy()[0][-1]\n",
    "\n",
    "            # sample next token and push it back into x_sequence\n",
    "            next_ix = np.random.choice(self.model.vocab_size, p=p_next)\n",
    "            next_ix = torch.tensor([[next_ix]])\n",
    "            if self.model.use_cuda:\n",
    "                next_ix = next_ix.cuda()\n",
    "            seed_ids = torch.cat((seed_ids, next_ix), dim=-1)\n",
    "            \n",
    "        seed_ids = seed_ids.cpu().data.numpy()[0]\n",
    "        restored_text = self.tokenizer.decode(seed_ids, skip_special_tokens=True)\n",
    "        restored_text = re.sub(r\"##\", \"\", restored_text)\n",
    "        return restored_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d6f825-082e-4d57-873a-8a2d162fdaa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thegpt = TheGirlPT.from_pretrained(pretrained_tokenizer=PRETRAINED_TOKENIZER,\n",
    "                                  model_config=MODEL_CONFIG,\n",
    "                                  pretrained_model=MODEL_WEIGTHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe61de34-565b-4826-8adc-c7b2d860a4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Нон-байнари тест, и мы скажем, как ты умрешь\n"
     ]
    }
   ],
   "source": [
    "print(thegpt.generate_sample(seed_phrase=\"Нон-байнари тест\", max_new_tokens=10, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e548675-dfcc-45d4-949a-767f236cf537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search-suggestions",
   "language": "python",
   "name": "search-suggestions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
